experiment:
  name: null 
  model: "deepconvnet"
  seed: 111
  dataset: "KU"
  optimizer: "AdamW"
  scheduler: null
  T_0: 2 
  T_mult: 2 # CosineAnnealingWarmRestarts hyperparameter
  device: "cuda"
  log_to_wandb: true
  wandb_project: "EEG-FM" #"lomso-experiments" # lomso-experiments
  n_subjects: 54 # 54 / 55 / 87
  checkpoint_dir: null # keep null, run_lomso will set it, otherwise it defaults to weights/checkpoints/<experiment_name>

  save_regular_checkpoints: false
  save_regular_checkpoints_interval: 5 # best val will also not save before this
  save_final_checkpoint: true
  save_best_checkpoints: true

  early_stopping: false
  early_stopping_patience: 5
  early_stopping_delta: 0.001


  epochs: 1
  train_after_stopping: true # if true, will train the model after early stopping
  train_after_stopping_epochs: 30

# KU
data:
  path: "data/preprocessed/KU_mi_labram_preprocessed_trial_normalized.h5"      # <-- set this
  electrodes: null               # if null/omitted, will use get_ku_dataset_channels()
  # choose exactly one of the two lines below:
  leave_out: #[20]             # explicit test subjects 
  m_leave_out: null #                  # OR: number of subjects to leave out at random, set to null for lomso
  train_proportion: 0.97   #adds up to 2 val subjs      # share of train_pool that goes to TRAIN (rest is VAL)
  subjects: null
  num_classes: 2 
  input_channels: 62
  samples: 800 # 800 for eegnet
  n_patches_labram: 4 # number of patches in the labram model
  patch_length: 200 # length of the patch in samples (trial_length / n_patches_labram)
  trial_length: 800 # length of the trial in samples
  fs: 200 # sampling frequency (downsampled from 1000 Hz)

labram:
  lr: 0.002
  weight_decay: 0.002
  dropout: null # 0.1 # dropout only when lora is false
  lora: true
  adapter_checkpoint_dir: null
  test_lr: 0.00005
  test_wd: 0.05

deepconvnet:
  lr: 0.001
  weight_decay: 0.0001
  test_lr: 0.0001
  test_wd: 0.001

peft_config:
  r: 2
  lora_alpha: 8
  lora_dropout: 0.5
  target_modules: ["proj","qkv","fc1"]

test:
  n_epochs: 20
  shots: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25] # number of shots per class
  n_repeats: 10
  model: "labram"
  model_folder_name: "labram-lora" # folder name where model is saved