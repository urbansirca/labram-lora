experiment:
  model: labram       # or "eegnet"
  device: cuda          # "cuda" | "cpu" | "mps"
  n_subjects: 54       # used if data.subjects not set
  seed: 111
  meta_iterations: 400
  validate_every: 30
  optimizer: AdamW       # Adam | AdamW
  scheduler: CosineAnnealingWarmRestarts   # CosineAnnealingLR | CosineAnnealingWarmRestarts | None
  # logging / checkpoints / early stopping
  log_to_wandb: true
  log_to_wandb_test: false
  wandb_entity: "urban-sirca-vrije-universiteit-amsterdam"
  wandb_project: "EEG-FM-META"


  save_regular_checkpoints: true
  save_final_checkpoint: true
  save_regular_checkpoints_interval: 100
  save_best_checkpoints: true
 
  # optional warm restarts extras
  T_0: 100 
  T_mult: 2
  eta_min: 1e-6

data:
  path: data/preprocessed/KU_mi_labram_preprocessed_trial_normalized.h5
  subjects: #[1,2,3,4]   # optional; else n_subjects used
  leave_out: [1,2,3,4,5,6]            # explicit test subjects 
  m_leave_out: #2 #                  # optional multi-LOO size
  train_proportion: 0.97
  n_channels: 62
  num_classes: 2
  # EEGLab/Labram shaping
  n_patches_labram: 4
  samples: 800                    # 800 for EEGNet (200 for Labram) #TODO: pofukani samples
  electrodes:                     # optional; falls back to get_ku_dataset_channels()

meta:
  meta_batch_size: 15
  k_support: 5
  q_query: 24         # or null for “all remaining”
  q_eval: 24          # or null for “all remaining”
  inner_steps: 3
  inner_lr: 3e-4
  run_size: 100       # granularity for episode indexing
  clip_grad_norm: 10.0  # torch.nn.utils.clip_grad_norm_
  val_episodes_per_subject: 1

supervised:
  train_batch_size: 500
  eval_batch_size: 500
  n_epochs_supervised: 20
  meta_iters_per_meta_epoch: 1
  validate_meta_every: 0
  drop_last: false
  run_size: 100       # granularity for episode indexing
  clip_grad_norm: null # torch.nn.utils.clip_grad_norm_
  val_episodes_per_subject: 1

optimizations:
  use_amp: false
  use_compile: false
  non_blocking: true
  pin_memory: false

# Model-specific blocks
labram:
  head_only_train: false # for test compatibility
  lora: true
  lr: 2e-4
  weight_decay: 0.0
  sup_lr: 2e-3
  sup_wd: 2e-3
  adapter_checkpoint_dir: #"/home/usirca/workspace/labram-lora/weights/checkpoints_meta/labram_AdamW_CosineAnnealingLR_114354/final_checkpoint_i800_acc0.688"
  test_lr: 0.00005
  test_wd: 0.05
  head_only_test: false

peft_config:
  r: 2
  lora_alpha: 8
  lora_dropout: 0.5
  target_modules: ["qkv", "fc1", "proj"]

eegnet:
  lr: 0.001
  weight_decay: 0.0
  dropoutRate: 0.5
  kernLength: 64
  F1: 8
  D: 2
  F2: 16
  sup_lr: 0.001
  sup_wd: 0.0
  test_lr: 0.0001
  test_wd: 0.001
  head_only_test: false 

deepconvnet:
  lr: 2e-4
  weight_decay: 0.0
  sup_lr: 2e-4
  sup_wd: 5e-4
  test_lr: 0.0001
  test_wd: 0.001
  head_only_test: false 
  
test:
  n_epochs: 2
  shots: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # number of shots for few-shot learning; if 0, will do standard testing without fine-tuning
  n_repeats: 3  # number of repeats for few-shot learning (with different random samples each time)
  models: ["labram"] # list of models to test; options are "deepconvnet", "eegnet", "labram"
  save_dir_root: "results_meta/lomso_3" # directory to save results and checkpoints; <experiment_name> will be replaced with the actual experiment name


lomso:
  max_folds: null # set to an integer to limit the number of folds for development purposes; if null, will use all folds
  test_only: true # if true, will only run testing without training

  train_root: "lomso_new_meta"                    # when test_only = false
  test_root: "test_only_lomso_new_meta"           # when test_only = true
  subdir: "alternating_run1"             # optional, appended to the root

  checkpoint_root: "lomso/alternating_run1"  # where trained ckpts live (used only if test_only)
  type: "best"                           # "best" | "final"

  use_cv_epoch_selection: false
  cv_min_shots: 6 