

experiment:
  model: "deepconvnet"
  seed: 111
  dataset: "KU"
  optimizer: "AdamW"
  scheduler: "CosineAnnealingLR"
  T_0: 2 # CosineAnnealingWarmRestarts hyperparameter
  T_mult: 2 # CosineAnnealingWarmRestarts hyperparameter
  device: "cuda"
  log_to_wandb: true
  n_subjects: 54

  save_regular_checkpoints: true
  save_regular_checkpoints_interval: 5
  save_final_checkpoint: true
  save_best_checkpoints: true

  early_stopping: false
  early_stopping_patience: 5
  early_stopping_delta: 0.001

  # train after stopping
  epochs: 100
  train_after_stopping: false # if true, will train the model after early stopping
  train_after_stopping_epochs: 3

optimizations:
  non_blocking: false
  pin_memory: false
  persistent_workers: false
  num_workers: 0 # if persistent_workers is true, then num_workers should be greater than 0
  use_amp: false # use automatic mixed precision - TURNING ON PREVENTS CONVERGENCE
  use_compile: false # use torch.compile


data:
  path: "data/preprocessed/KU_mi_labram_preprocessed.h5"      # <-- set this
  electrodes: ["FP1","FP2","F7","F3","FZ","F4","F8","FC5","FC1","FC2","FC6",
    "T7","C3","CZ","C4","T8","TP9","CP5","CP1","CP2","CP6","TP10","P7","P3","PZ",
    "P4","P8","PO9","O1","OZ","O2","PO10","FC3","FC4","C5","C1","C2","C6","CP3",
    "CPZ","CP4","P1","P2","POZ","FT9","FTT9h","TTP7h","TP7","TPP9h","FT10","FTT10h",
    "TPP8h","TP8","TPP10h","F9","F10","AF7","AF3","AF4","AF8","PO3","PO4"]


  # choose exactly one of the two lines below:
  # leave_out: [1, 2]             # explicit test subjects 
  m_leave_out: 2                  # OR: number of subjects to leave out at random
  train_proportion: 0.90          # share of train_pool that goes to TRAIN (rest is VAL)
  subjects: null                  # if null/omitted, will use experiment.subjects_ids_list
  num_classes: 2 
  input_channels: 62
  samples: 800 # 800 for eegnet
  n_patches_labram: 4 # number of patches in the labram model
  patch_length: 200 # length of the patch in samples (trial_length / n_patches_labram)
  trial_length: 800 # length of the trial in samples
  fs: 200 # sampling frequency (downsampled from 1000 Hz)

sampler:
  train_batch_size: 250
  eval_batch_size: 250
  drop_last: false
  shuffle_subjects: true
  shuffle_trials: true

labram:
  lr: 0.002
  weight_decay: 0.002
  seed: 42
  lora: true

peft_config: # Roomier budget (≈1–2% trainable, often best trade-off)
  r: 2
  lora_alpha: 32
  lora_dropout: 0.5
  # target_modules: [
  #   "blocks.0.attn.qkv", "blocks.1.attn.qkv", "blocks.2.attn.qkv", "blocks.3.attn.qkv", "blocks.4.attn.qkv", "blocks.5.attn.qkv",
  #   "blocks.6.attn.qkv", "blocks.7.attn.qkv", "blocks.8.attn.qkv", "blocks.9.attn.qkv", "blocks.10.attn.qkv", "blocks.11.attn.qkv",
  #   "blocks.0.attn.proj", "blocks.1.attn.proj", "blocks.2.attn.proj", "blocks.3.attn.proj", "blocks.4.attn.proj", "blocks.5.attn.proj",
  #   "blocks.6.attn.proj", "blocks.7.attn.proj", "blocks.8.attn.proj", "blocks.9.attn.proj", "blocks.10.attn.proj", "blocks.11.attn.proj",
  #   "blocks.0.mlp.fc1", "blocks.1.mlp.fc1", "blocks.2.mlp.fc1", "blocks.3.mlp.fc1", "blocks.4.mlp.fc1", "blocks.5.mlp.fc1",
  #   "blocks.6.mlp.fc1", "blocks.7.mlp.fc1", "blocks.8.mlp.fc1", "blocks.9.mlp.fc1", "blocks.10.mlp.fc1", "blocks.11.mlp.fc1"
  # ]
  target_modules: [   
    "proj",
    "qkv",
    "fc1"
  ]

eegnet:
  lr: 0.001
  weight_decay: 0.0
  dropoutRate: 0.5
  kernLength: 64
  F1: 8
  D: 2
  F2: 16


