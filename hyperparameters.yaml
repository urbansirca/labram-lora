

experiment:
  model: "labram" # later deepConvNet or EEGNet
  epochs: 100
  warmup_epochs: 0
  seed: 111
  LOMSO_folds: 6
  n_fold: "all" # or 1,2,3,4,5 #TODO: implement this
  dataset: "KU"
  meta: true
  optimizer: "AdamW"
  scheduler: "CosineAnnealingLR"
  device: "cuda"
  shuffled_subjects:  [35, 47, 46, 37, 13, 27, 12, 32, 53, 54, 4, 40, 19, 41, 18, 42, 34, 7, 49, 9, 5, 48, 29, 15, 21, 17, 31, 45, 1, 38, 51, 8, 11, 16, 28, 44, 24, 52, 3, 26, 39, 50, 6, 23, 2, 14, 25, 20, 10, 33, 22, 43, 36, 30]



labram:
  lr: 0.001
  weight_decay: 0.001
  batch_size: 32
  seed: 42
  meta_lr: 0.001
  lora: true

peft_config: # Roomier budget (≈1–2% trainable, often best trade-off)
  r: 2
  lora_alpha: 32
  lora_dropout: 0.5
  target_modules: [
    "blocks.0.attn.qkv", "blocks.1.attn.qkv", "blocks.2.attn.qkv", "blocks.3.attn.qkv", "blocks.4.attn.qkv", "blocks.5.attn.qkv",
    "blocks.6.attn.qkv", "blocks.7.attn.qkv", "blocks.8.attn.qkv", "blocks.9.attn.qkv", "blocks.10.attn.qkv", "blocks.11.attn.qkv",
    "blocks.0.attn.proj", "blocks.1.attn.proj", "blocks.2.attn.proj", "blocks.3.attn.proj", "blocks.4.attn.proj", "blocks.5.attn.proj",
    "blocks.6.attn.proj", "blocks.7.attn.proj", "blocks.8.attn.proj", "blocks.9.attn.proj", "blocks.10.attn.proj", "blocks.11.attn.proj",
    "blocks.0.mlp.fc1", "blocks.1.mlp.fc1", "blocks.2.mlp.fc1", "blocks.3.mlp.fc1", "blocks.4.mlp.fc1", "blocks.5.mlp.fc1",
    "blocks.6.mlp.fc1", "blocks.7.mlp.fc1", "blocks.8.mlp.fc1", "blocks.9.mlp.fc1", "blocks.10.mlp.fc1", "blocks.11.mlp.fc1"
  ]



deepconvnet:
  lr: 0.001
  batch_size: 32
  seed: 42
  num_classes: 2
  input_channels: 22
  input_rows: 1


