experiment:
  model: labram         # or "eegnet"
  device: cuda          # "cuda" | "cpu" | "mps"
  n_subjects: 54       # used if data.subjects not set
  seed: 111
  epochs: 30
  optimizer: AdamW       # Adam | AdamW
  scheduler: CosineAnnealingLR   # CosineAnnealingLR | CosineAnnealingWarmRestarts | None
  # logging / checkpoints / early stopping
  log_to_wandb: true
  wandb_entity: "urban-sirca-vrije-universiteit-amsterdam"
  wandb_project: "EEG-FM"


  save_regular_checkpoints: false
  save_final_checkpoint: true
  save_best_checkpoints: true
  save_regular_checkpoints_interval: 5
 
  # optional warm restarts extras
  T_0: 2
  T_mult: 2

data:
  path: data/preprocessed/KU_mi_labram_preprocessed.h5
  # subjects: [1,2,3,4,5,6,7,8,9,10]   # optional; else n_subjects used
  # leave_out: [1,2,3,4,5,6]                # test set subjects
  m_leave_out: 2                  # optional multi-LOO size
  train_proportion: 0.9
  input_channels: 62
  num_classes: 2
  # EEGLab/Labram shaping
  n_patches_labram: 4
  patch_length: 200
  samples: 200                    # 800 for EEGNet (200 for Labram)
  electrodes:                     # optional; falls back to get_ku_dataset_channels()

meta:
  meta_batch_size: 10
  k_support: 5
  q_query: 10         # or null for “all remaining”
  inner_steps: 2
  inner_lr: 7e-5
  steps_per_epoch: 100
  run_size: 100       # granularity for episode indexing
  clip_grad_norm: 3.5

optimizations:
  use_amp: false
  use_compile: false
  non_blocking: true
  pin_memory: false

# Model-specific blocks
# labram:
#   lora: true
#   lr: 3.910517525546074e-05
#   weight_decay: 0.003035567355119174
#   adapter_checkpoint_dir: "/home/usirca/workspace/labram-lora/weights/checkpoints/labram_lr0.002_wd0.002_AdamW_CosineAnnealingLR_131222/checkpoint__e20_acc0.816"
#   # adapter_checkpoint_dir: "/home/usirca/workspace/labram-lora/weights/checkpoints/looped_runs/labram_leave_out1-6_lr0.002_wd0.002_AdamW_CosineAnnealingLR_224552/best_val_checkpoint"

labram:
  lora: true
  lr: 2e-4            # “base” (for any decay group)
  lora_lr: 3e-4       # adapters, biases, norms (no decay)
  head_lr: 2e-4
  weight_decay: 1e-4  # decay group
  head_weight_decay: 5e-4
  # adapter_checkpoint_dir: "/home/usirca/workspace/labram-lora/weights/checkpoints/labram_lr0.002_wd0.002_AdamW_CosineAnnealingLR_131222/checkpoint__e20_acc0.816"
  adapter_checkpoint_dir: "/home/usirca/workspace/labram-lora/weights/checkpoints/labram_lr0.002_wd0.002_AdamW_CosineAnnealingLR_131222/checkpoint__e10_acc0.686"


peft_config:
  r: 2
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["qkv", "fc1", "proj"]

eegnet:
  lr: 0.001
  weight_decay: 0.0
  dropoutRate: 0.5
  kernLength: 64
  F1: 8
  D: 2
  F2: 16

test:
  shots_list: [0, 1, 2, 3, 4, 5, 10, 25, 50]
  n_epochs: 10
  save_dir: "results/test"