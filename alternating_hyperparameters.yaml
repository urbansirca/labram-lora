# ============================================================
# One YAML to drive both NORMAL (supervised) and META configs
# Uses anchors & merges to avoid duplication
# ============================================================

experiment_name: "alternating_train"

# ---------- shared knobs (anchored) ----------
shared:
  experiment_common: &exp_common
    model: deepconvnet           # or "eegnet" or "labram"
    device: cuda                 # "cuda" | "cpu" | "mps"
    n_subjects: 54
    seed: 111
    log_to_wandb: true
    wandb_entity: "urban-sirca-vrije-universiteit-amsterdam"
    wandb_project: "EEG-FM"

    save_regular_checkpoints: true
    save_final_checkpoint: true

  data_common: &data_common
    path: "data/preprocessed/KU_mi_labram_preprocessed.h5"
    # subjects: [1,2,3,4,5,6,7,8,9,10]        # optional; else n_subjects used
    leave_out: [1,2,3,4,5,6]                  # test set subjects
    # m_leave_out: 2                           # alternative: multi-LOO size
    train_proportion: 0.9
    input_channels: 62
    num_classes: 2
    n_patches_labram: 4
    patch_length: 200
    samples: 200
    trial_length: 800
    electrodes: ["FP1","FP2","F7","F3","FZ","F4","F8","FC5","FC1","FC2","FC6", "T7","C3","CZ","C4","T8","TP9","CP5","CP1","CP2","CP6","TP10","P7","P3","PZ", "P4","P8","PO9","O1","OZ","O2","PO10","FC3","FC4","C5","C1","C2","C6","CP3", "CPZ","CP4","P1","P2","POZ","FT9","FTT9h","TTP7h","TP7","TPP9h","FT10","FTT10h", "TPP8h","TP8","TPP10h","F9","F10","AF7","AF3","AF4","AF8","PO3","PO4"]
     
  optimizations_common: &opt_common
    use_amp: false
    use_compile: false
    non_blocking: true
    pin_memory: false
    persistent_workers: false
    num_workers: 0

  peft: &peft_config
    r: 2
    lora_alpha: 32
    lora_dropout: 0.1
    target_modules: ["proj","qkv","fc1"]

  eegnet_hp: &eegnet_hp
    lr: 0.001
    weight_decay: 0.0
    dropoutRate: 0.5
    kernLength: 64
    F1: 8
    D: 2
    F2: 16

  deepconvnet_hp: &deepconvnet_hp
    lr: 0.001
    weight_decay: 0.0

  labram_hp: &labram_hp
    lora: true
    # lr: 2e-4
    # weight_decay: 1e-4
    lora_lr: 2e-4
    head_lr: 5e-4
    head_weight_decay: 5e-4
    adapter_checkpoint_dir: "/home/usirca/workspace/labram-lora/weights/checkpoints/labram_lr0.002_wd0.002_AdamW_CosineAnnealingLR_191909/checkpoint__e10_acc0.565"


# ============================================================
# TRAIN (NORMAL / supervised) — matches train.get_engine()
# ============================================================
train:
  experiment:
    <<: *exp_common
    optimizer: "AdamW"
    scheduler: "CosineAnnealingLR"   # or "CosineAnnealingWarmRestarts" | "None"
    T_0: 2
    T_mult: 2

    # early stopping / epochs / TAS
    epochs: 100
    early_stopping: false
    early_stopping_patience: 5
    early_stopping_delta: 0.001
    train_after_stopping: false
    train_after_stopping_epochs: 3

    save_regular_checkpoints_interval: 5
    save_best_checkpoints: true

  data:
    <<: *data_common
    # choose exactly one:
    # leave_out: [1,2]        # explicit test subjects
    # m_leave_out: 2           # or random LOO size

  sampler:
    train_batch_size: 250
    eval_batch_size: 250
    drop_last: false
    shuffle_subjects: true
    shuffle_trials: true

  optimizations:
    <<: *opt_common
    non_blocking: false  # override for supervised if desired

  # model-specific hyperparameters (used by get_engine)
  eegnet: *eegnet_hp
  deepconvnet: *deepconvnet_hp
  labram:
    <<: *labram_hp
    seed: 42
  peft_config: *peft_config


# ============================================================
# META (meta-learning) — matches meta_train.get_meta_engine()
# ============================================================
meta:
  experiment:
    <<: *exp_common
    meta_iterations: 1500
    validate_every: 30
    optimizer: AdamW
    scheduler: CosineAnnealingLR
    save_regular_checkpoints_interval: 100
    # warm restarts extras (used only if scheduler == CosineAnnealingWarmRestarts)
    T_0: 100
    T_mult: 2
    eta_min: 1e-5

  data:
    <<: *data_common
    # keep the same split policy as TRAIN for strict equality checks
    # leave_out: [1,2,3,4,5,6]
    # m_leave_out: 2

  meta:
    meta_batch_size: 10
    k_support: 5
    q_query: 10
    q_eval: 30
    inner_steps: 2
    inner_lr: 3e-5
    run_size: 100
    clip_grad_norm: 3.5
    val_episodes_per_subject: 1

  optimizations:
    <<: *opt_common

  # model-specific hyperparameters (used by get_meta_engine)
  eegnet: *eegnet_hp
  deepconvnet: *deepconvnet_hp
  labram: *labram_hp
  peft_config: *peft_config

  test:
    shots_list: [0, 1, 2, 3, 4, 5, 10, 25, 50]
    n_epochs: 10
    save_dir: "results/test"


# ============================================================
# Alternating loop knobs for the combined script
# ============================================================
alternating:
  total_epochs: 50                 # cycles; each = 1 supervised epoch + 1 meta "epoch"
  meta_iters_per_meta_epoch: 50    # meta-iterations per meta epoch
  validate_meta_every: 1           # meta validate frequency within meta epoch
  with_tester: false               # set true to create TestEngines
  test_after: false                # run testers after alternating training
